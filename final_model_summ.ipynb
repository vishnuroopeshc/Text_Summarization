{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ed7f6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "2025-05-10 08:50:42 INFO: Writing properties to tmp file: corenlp_server-80300291fff34cf4.props\n",
      "2025-05-10 08:50:42 INFO: Starting server with command: java -Xmx6G -cp /Users/abhinavkrishna/stanza_corenlp/* edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9500 -timeout 30000 -threads 5 -maxCharLength 100000 -quiet True -serverProperties corenlp_server-80300291fff34cf4.props -annotators tokenize,ssplit,pos,lemma,ner,depparse,openie -preload -outputFormat serialized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting triplets...\n",
      "Normalizing triplets...\n",
      "Storing in knowledge graph...\n",
      "Processing query: Tesla\n",
      "Detected entities in query: Model X. Tesla 'S Gigafactory, Model X. Tesla, Tesla Inc.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/utils/extmath.py:203: RuntimeWarning: divide by zero encountered in matmul\n",
      "  ret = a @ b\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/utils/extmath.py:203: RuntimeWarning: overflow encountered in matmul\n",
      "  ret = a @ b\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/utils/extmath.py:203: RuntimeWarning: invalid value encountered in matmul\n",
      "  ret = a @ b\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/utils/extmath.py:203: RuntimeWarning: divide by zero encountered in matmul\n",
      "  ret = a @ b\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/utils/extmath.py:203: RuntimeWarning: overflow encountered in matmul\n",
      "  ret = a @ b\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/utils/extmath.py:203: RuntimeWarning: invalid value encountered in matmul\n",
      "  ret = a @ b\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top matching entities:\n",
      "- Tesla Inc. (score: 0.802)\n",
      "- Model X. Tesla (score: 0.729)\n",
      "- Model X. Tesla 'S Gigafactory (score: 0.574)\n",
      "- its electric vehicles (score: 0.561)\n",
      "- world 's car manufacturer (score: 0.534)\n",
      "\n",
      "Top matching relationships:\n",
      "- Tesla Inc. became world 's car manufacturer (score: 0.684)\n",
      "- Tesla Inc. became_in 2022 (score: 0.678)\n",
      "- Tesla Inc. became world 's valuable car manufacturer (score: 0.678)\n",
      "- Tesla Inc. became world 's most valuable car manufacturer (score: 0.670)\n",
      "- Tesla Inc. has has integrated into several applications (score: 0.621)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/utils/extmath.py:203: RuntimeWarning: divide by zero encountered in matmul\n",
      "  ret = a @ b\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/utils/extmath.py:203: RuntimeWarning: overflow encountered in matmul\n",
      "  ret = a @ b\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/utils/extmath.py:203: RuntimeWarning: invalid value encountered in matmul\n",
      "  ret = a @ b\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top matching relationships:\n",
      "- Model X. Tesla 'S Gigafactory headquartered_in Nevada (score: 1.000)\n",
      "- Model X. Tesla 'S Gigafactory is_in Nevada (score: 1.000)\n",
      "- Model X. Tesla has Gigafactory in Nevada (score: 1.000)\n",
      "- Tesla Inc. became world 's most valuable car manufacturer (score: 1.000)\n",
      "- Tesla Inc. has has integrated into several applications including Microsoft 's Copilot (score: 1.000)\n",
      "- Tesla Inc. manufactures_in collaboration with Foxconn (score: 1.000)\n",
      "- Tesla Inc. has has integrated into applications including Microsoft 's Copilot (score: 1.000)\n",
      "- Tesla Inc. has has integrated into applications (score: 1.000)\n",
      "- Tesla Inc. became world 's valuable car manufacturer (score: 1.000)\n",
      "- Tesla Inc. has has integrated (score: 1.000)\n",
      "\n",
      "Found 15 relevant relationships:\n",
      "- Model X. Tesla 'S Gigafactory headquartered_in Nevada (score: 1.000)\n",
      "- Model X. Tesla 'S Gigafactory is_in Nevada (score: 1.000)\n",
      "- Model X. Tesla has Gigafactory in Nevada (score: 1.000)\n",
      "- Tesla Inc. became world 's most valuable car manufacturer (score: 1.000)\n",
      "- Tesla Inc. has has integrated into several applications including Microsoft 's Copilot (score: 1.000)\n",
      "- Tesla Inc. manufactures_in collaboration with Foxconn (score: 1.000)\n",
      "- Tesla Inc. has has integrated into applications including Microsoft 's Copilot (score: 1.000)\n",
      "- Tesla Inc. has has integrated into applications (score: 1.000)\n",
      "- Tesla Inc. became world 's valuable car manufacturer (score: 1.000)\n",
      "- Tesla Inc. has has integrated (score: 1.000)\n",
      "\n",
      "Query-specific summary: Tesla 'S Gigafactory is in Nevada - Tesla Inc. became world 's most valuable car manufacturer -. - has integrated into several applications including. microsoft's copilot - manufactures in collaboration with Foxconn - and uses machine learning algorithms.\n",
      "\n",
      "Summary: Tesla 'S Gigafactory is in Nevada - Tesla Inc. became world 's most valuable car manufacturer -. - has integrated into several applications including. microsoft's copilot - manufactures in collaboration with Foxconn - and uses machine learning algorithms.\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "from stanza.server import CoreNLPClient\n",
    "import re\n",
    "from neo4j import GraphDatabase\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "class KnowledgeGraphPipeline:\n",
    "    def __init__(self):\n",
    "        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        self.neo4j_url = \"bolt://localhost:7687\"\n",
    "        self.neo4j_user = \"neo4j\"\n",
    "        self.neo4j_password = \"Author123\"\n",
    "        self.driver = GraphDatabase.driver(\n",
    "            self.neo4j_url, \n",
    "            auth=(self.neo4j_user, self.neo4j_password)\n",
    "        )\n",
    "        self.summarization_model_name = \"t5-large\"\n",
    "        self.tokenizer = T5Tokenizer.from_pretrained(self.summarization_model_name)\n",
    "        self.summarization_model = T5ForConditionalGeneration.from_pretrained(self.summarization_model_name)\n",
    "    \n",
    "    def extract_triplets(self, text):\n",
    "        with CoreNLPClient(\n",
    "            annotators=['tokenize', 'ssplit', 'pos', 'lemma', 'ner', 'depparse', 'openie'],\n",
    "            be_quiet=True,\n",
    "            endpoint='http://localhost:9500',\n",
    "            memory='6G',\n",
    "            timeout=30000\n",
    "        ) as client:\n",
    "            ann = client.annotate(text)\n",
    "            \n",
    "            triplets = []\n",
    "            for sentence in ann.sentence:\n",
    "                for triple in sentence.openieTriple:\n",
    "                    subj = triple.subject\n",
    "                    rel = triple.relation\n",
    "                    obj = triple.object\n",
    "                    conf = triple.confidence\n",
    "                    triplets.append((subj, rel, obj, conf))\n",
    "            \n",
    "            return triplets\n",
    "    \n",
    "    def normalize_triplets(self, raw_triplets, confidence_threshold=0.85):\n",
    "        cleaned = set()\n",
    "        for triplet in raw_triplets:\n",
    "            if len(triplet) == 4:\n",
    "                subj, rel, obj, conf = triplet\n",
    "            else:\n",
    "                subj, rel, obj = triplet\n",
    "                conf = 1.0\n",
    "\n",
    "            if conf < confidence_threshold:\n",
    "                continue\n",
    "\n",
    "            subj = subj.strip().title()\n",
    "            rel = rel.strip().lower()\n",
    "            obj = obj.strip()\n",
    "\n",
    "            if subj.lower() in [\"it\", \"the company\"]:\n",
    "                subj = \"Tesla Inc.\"  \n",
    "            if subj.lower() == \"he\":\n",
    "                subj = \"Jeff Bezos\"\n",
    "            if subj.lower() == \"she\":\n",
    "                subj = \"Tim Cook\"\n",
    "\n",
    "            if rel in [\"was\", \"is\", \"are\"] or obj.lower() in [\"awarded\", \"born\", \"founded\"]:\n",
    "                continue\n",
    "\n",
    "            rel = rel.replace(\" \", \"_\")\n",
    "            rel = re.sub(r\"\\b(in|on|to|by|as|from)\\b\", \"\", rel).strip(\"_\")\n",
    "\n",
    "            if \"was_awarded_to\" in rel:\n",
    "                cleaned.add((subj, \"awarded_to\", obj))\n",
    "            elif \"was_awarded_in\" in rel:\n",
    "                cleaned.add((subj, \"award_year\", obj))\n",
    "            elif \"was_awarded\" in rel:\n",
    "                match = re.search(r\"(.+?) in (\\d{4})\", obj)\n",
    "                if match:\n",
    "                    cleaned.add((subj, \"awarded_to\", match.group(1).strip()))\n",
    "                    cleaned.add((subj, \"award_year\", match.group(2).strip()))\n",
    "                else:\n",
    "                    cleaned.add((subj, \"awarded_to\", obj))\n",
    "\n",
    "            elif \"born_in\" in rel:\n",
    "                cleaned.add((subj, \"born_in\", obj))\n",
    "            elif \"born_on\" in rel:\n",
    "                cleaned.add((subj, \"birth_date\", obj))\n",
    "            elif \"founded_by\" in rel:\n",
    "                cleaned.add((subj, \"founded_by\", obj))\n",
    "            elif \"founded_in\" in rel:\n",
    "                cleaned.add((subj, \"founded_in\", obj))\n",
    "\n",
    "            elif \"headquartered_in\" in rel or \"located_in\" in rel or \"based_in\" in rel or \"is_in\" in rel:\n",
    "                cleaned.add((subj, \"headquartered_in\", obj))\n",
    "\n",
    "            year_match = re.search(r\"(.+?) in (\\d{4})\", obj)\n",
    "            if year_match:\n",
    "                cleaned.add((subj, rel, year_match.group(1).strip()))\n",
    "                cleaned.add((subj, f\"{rel}_year\", year_match.group(2).strip()))\n",
    "            else:\n",
    "                cleaned.add((subj, rel, obj))\n",
    "\n",
    "            if subj == \"Tim Cook\" and \"succeeded\" in rel and \"Steve Jobs\" in obj:\n",
    "                if \"as\" in obj:\n",
    "                    cleaned.add((subj, \"succeeded_steve_jobs_as\", \"CEO\"))\n",
    "                elif \"in\" in obj:\n",
    "                    year = re.findall(r\"\\d{4}\", obj)\n",
    "                    if year:\n",
    "                        cleaned.add((subj, \"succeeded_steve_jobs_in\", year[0]))\n",
    "                    cleaned.add((subj, \"succeeded\", \"Steve Jobs\"))\n",
    "                else:\n",
    "                    cleaned.add((subj, \"succeeded\", \"Steve Jobs\"))\n",
    "\n",
    "            if subj == \"Jeff Bezos\" and \"stepped\" in rel:\n",
    "                if \"down as\" in rel:\n",
    "                    cleaned.add((subj, \"stepped_down_as\", obj))\n",
    "                if \"down in\" in rel:\n",
    "                    cleaned.add((subj, \"stepped_down_in\", obj))\n",
    "                if \"as\" in rel:\n",
    "                    cleaned.add((subj, \"stepped_as\", obj))\n",
    "\n",
    "        return list(cleaned)\n",
    "    \n",
    "    def store_graph_embeddings(self, relations):\n",
    "        with self.driver.session() as session:\n",
    "            session.run(\"MATCH (n) DETACH DELETE n\")\n",
    "            \n",
    "            session.run(\"\"\"UNWIND $relations AS rel\n",
    "                        MERGE (source:Entity {name: rel[0]})\n",
    "                        MERGE (target:Entity {name: rel[2]})\n",
    "                        MERGE (source)-[r:RELATIONSHIP {type: rel[1]}]->(target)\n",
    "                        \"\"\", relations=relations)\n",
    "            \n",
    "            nodes = session.run(\"MATCH (e:Entity) RETURN e.name as name\").data()\n",
    "            for node in nodes:\n",
    "                embedding = self.embedding_model.encode(node['name']).tolist()\n",
    "                session.run(\"\"\"\n",
    "                    MATCH (e:Entity {name: $name})\n",
    "                    SET e.embedding = $embedding\n",
    "                    \"\"\", name=node['name'], embedding=embedding)\n",
    "            \n",
    "            rels = session.run(\"\"\"\n",
    "                MATCH (s)-[r:RELATIONSHIP]->(t)\n",
    "                RETURN s.name AS source, r.type AS type, t.name AS target\n",
    "                \"\"\").data()\n",
    "            \n",
    "            for rel in rels:\n",
    "                text = f\"{rel['source']} {rel['type']} {rel['target']}\"\n",
    "                embedding = self.embedding_model.encode(text).tolist()\n",
    "                session.run(\"\"\"\n",
    "                    MATCH (s {name: $source})-[r:RELATIONSHIP {type: $type}]->(t {name: $target})\n",
    "                    SET r.embedding = $embedding\n",
    "                    \"\"\", \n",
    "                    source=rel['source'], type=rel['type'], \n",
    "                    target=rel['target'], embedding=embedding)\n",
    "    \n",
    "    def create_vector_indexes(self):\n",
    "        with self.driver.session() as session:\n",
    "            session.run(\"\"\"\n",
    "                CREATE VECTOR INDEX entity_embeddings IF NOT EXISTS\n",
    "                FOR (e:Entity) ON e.embedding\n",
    "                OPTIONS {indexConfig: {\n",
    "                  `vector.dimensions`: 384,\n",
    "                  `vector.similarity_function`: 'cosine'\n",
    "                }}\n",
    "            \"\"\")\n",
    "            \n",
    "            session.run(\"\"\"\n",
    "                CREATE VECTOR INDEX relationship_embeddings IF NOT EXISTS\n",
    "                FOR ()-[r:RELATIONSHIP]-() ON r.embedding\n",
    "                OPTIONS {indexConfig: {\n",
    "                  `vector.dimensions`: 384,\n",
    "                  `vector.similarity_function`: 'cosine'\n",
    "                }}\n",
    "            \"\"\")\n",
    "    \n",
    "    def semantic_graph_search(self, query):\n",
    "        query_embedding = self.embedding_model.encode(query).reshape(1, -1)\n",
    "        \n",
    "        with self.driver.session() as session:\n",
    "\n",
    "            nodes = session.run(\"MATCH (e:Entity) RETURN e.name as name, e.embedding as embedding\").data()\n",
    "            \n",
    "            if nodes:\n",
    "                names = [n['name'] for n in nodes]\n",
    "                embeddings = [n['embedding'] for n in nodes]\n",
    "                similarities = cosine_similarity(query_embedding, embeddings)[0]\n",
    "                \n",
    "                print(\"Top matching entities:\")\n",
    "                for idx in np.argsort(similarities)[-5:][::-1]:\n",
    "                    print(f\"- {names[idx]} (score: {similarities[idx]:.3f})\")\n",
    "        \n",
    "\n",
    "            rels = session.run(\"\"\"\n",
    "                MATCH (s)-[r:RELATIONSHIP]->(t)\n",
    "                RETURN s.name as source, r.type as type, t.name as target, r.embedding as embedding\n",
    "                \"\"\").data()\n",
    "            \n",
    "            if rels:\n",
    "                triples = [f\"{r['source']} {r['type']} {r['target']}\" for r in rels]\n",
    "                embeddings = [r['embedding'] for r in rels]\n",
    "                similarities = cosine_similarity(query_embedding, embeddings)[0]\n",
    "                \n",
    "                print(\"\\nTop matching relationships:\")\n",
    "                for idx in np.argsort(similarities)[-5:][::-1]:\n",
    "                    print(f\"- {triples[idx]} (score: {similarities[idx]:.3f})\")\n",
    "                top_indices = np.argsort(similarities)[-10:][::-1]\n",
    "                return [(triples[idx], similarities[idx], rels[idx]) for idx in top_indices]\n",
    "        \n",
    "        return []\n",
    "    \n",
    "    def generate_summary(self, num_key_relations=10, query=None, relations=None):\n",
    "        if query and not relations:\n",
    "            relations_data = self.retrieve_query_relevant_relations(query)\n",
    "            if not relations_data:\n",
    "                return \"No relevant information found for this query.\"\n",
    "        elif not relations:\n",
    "            with self.driver.session() as session:\n",
    "                relations_data = session.run(\"\"\"\n",
    "                    MATCH (s)-[r:RELATIONSHIP]->(t)\n",
    "                    RETURN s.name as source, r.type as type, t.name as target\n",
    "                    LIMIT $limit\n",
    "                    \"\"\", limit=num_key_relations).data()\n",
    "        else:\n",
    "            relations_data = relations\n",
    "        formatted_relations = []\n",
    "        \n",
    "        entity_context = {}\n",
    "        seen_content = set()  \n",
    "        \n",
    "        if isinstance(relations_data[0], tuple) and len(relations_data[0]) >= 3:\n",
    "            for rel in relations_data:\n",
    "                if isinstance(rel[2], dict):\n",
    "                    source = rel[2]['source']\n",
    "                    rel_type = rel[2]['type']\n",
    "                    target = rel[2]['target']\n",
    "                else:\n",
    "                    rel_parts = rel[0].split()\n",
    "                    if len(rel_parts) >= 3:\n",
    "                        source = rel_parts[0]\n",
    "                        rel_type = rel_parts[1]\n",
    "                        target = ' '.join(rel_parts[2:])\n",
    "                    else:\n",
    "                        continue \n",
    "                \n",
    "                relation_text = f\"{source} {rel_type.replace('_', ' ')} {target}\"\n",
    "                if relation_text.lower() not in seen_content:\n",
    "                    formatted_relations.append(relation_text)\n",
    "                    seen_content.add(relation_text.lower())\n",
    "                    \n",
    "\n",
    "                    if source not in entity_context:\n",
    "                        entity_context[source] = []\n",
    "                    entity_context[source].append((rel_type, target))\n",
    "        else:\n",
    "            for rel in relations_data:\n",
    "                relation_text = f\"{rel['source']} {rel['type'].replace('_', ' ')} {rel['target']}\"\n",
    "                if relation_text.lower() not in seen_content:\n",
    "                    formatted_relations.append(relation_text)\n",
    "                    seen_content.add(relation_text.lower())\n",
    "                    if rel['source'] not in entity_context:\n",
    "                        entity_context[rel['source']] = []\n",
    "                    entity_context[rel['source']].append((rel['type'], rel['target']))\n",
    "        \n",
    "        facts = []\n",
    "        \n",
    "        for entity, relations in entity_context.items():\n",
    "            entity_facts = []\n",
    "            for rel_type, target in relations:\n",
    "                fact = f\"{entity} {rel_type.replace('_', ' ')} {target}\"\n",
    "                entity_facts.append(fact)\n",
    "            \n",
    "            facts.extend(entity_facts)\n",
    "        \n",
    "        if len(facts) < 5 and query:\n",
    "            with self.driver.session() as session:\n",
    "                additional_rels = session.run(\"\"\"\n",
    "                    MATCH (s)-[r:RELATIONSHIP]->(t)\n",
    "                    WHERE toLower(s.name) CONTAINS toLower($query) OR \n",
    "                          toLower(t.name) CONTAINS toLower($query) OR\n",
    "                          toLower(r.type) CONTAINS toLower($query)\n",
    "                    RETURN s.name as source, r.type as type, t.name as target\n",
    "                    LIMIT 10\n",
    "                \"\"\", query=query).data()\n",
    "                \n",
    "                for rel in additional_rels:\n",
    "                    fact = f\"{rel['source']} {rel['type'].replace('_', ' ')} {rel['target']}\"\n",
    "                    if fact.lower() not in seen_content:\n",
    "                        facts.append(fact)\n",
    "                        seen_content.add(fact.lower())\n",
    "        \n",
    "\n",
    "        if not facts:\n",
    "            return f\"I don't have enough information about {query} in my knowledge graph.\"\n",
    "            \n",
    "        facts_text = \"\\n\".join([f\"- {fact}\" for fact in facts])\n",
    "        \n",
    "        if query:\n",
    "            input_text = f\"\"\"\n",
    "            summarize: Create a coherent paragraph about {query} based on these facts:\n",
    "            \n",
    "            {facts_text}\n",
    "            \n",
    "            Write a natural-sounding paragraph that summarizes the key information. \n",
    "            \"\"\"\n",
    "        else:\n",
    "            input_text = f\"\"\"\n",
    "            summarize: Create a coherent paragraph based on these facts:\n",
    "            \n",
    "            {facts_text}\n",
    "            \n",
    "            Write a natural-sounding paragraph that summarizes the key information. \n",
    "            \"\"\"\n",
    "        \n",
    "\n",
    "        inputs = self.tokenizer.encode(input_text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "        \n",
    "\n",
    "        outputs = self.summarization_model.generate(\n",
    "            inputs,\n",
    "            max_length=200,\n",
    "            min_length=50,\n",
    "            num_beams=8,\n",
    "            length_penalty=2.0,\n",
    "            early_stopping=True,\n",
    "            no_repeat_ngram_size=3\n",
    "        )\n",
    "        \n",
    "        summary = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        summary = summary.replace(\"Summary:\", \"\").strip()\n",
    "        \n",
    "        if summary and len(summary) > 0:\n",
    "            summary = summary[0].upper() + summary[1:]\n",
    "        \n",
    "        instruction_patterns = [\n",
    "            \"focus on key details\", \n",
    "            \"avoid repetition\", \n",
    "            \"present the information\", \n",
    "            \"coherent narrative\", \n",
    "            \"write a natural\",\n",
    "            \"summarize:\",\n",
    "            \"create a coherent\"\n",
    "        ]\n",
    "        \n",
    "        for pattern in instruction_patterns:\n",
    "            if summary.lower().startswith(pattern):\n",
    "                first_period = summary.find('.', summary.lower().find(pattern))\n",
    "                if first_period > 0:\n",
    "                    summary = summary[first_period+1:].strip()\n",
    "        \n",
    "        if len(summary) > 50:\n",
    "            first_half = summary[:len(summary)//2]\n",
    "            second_half = summary[len(summary)//2:]\n",
    "            \n",
    "            if first_half.strip() == second_half.strip():\n",
    "                summary = first_half\n",
    "        \n",
    "        return summary\n",
    "    \n",
    "    def retrieve_query_relevant_relations(self, query, threshold=0.5, max_relations=15):\n",
    "        \"\"\"\n",
    "        Retrieve relations from the knowledge graph that are relevant to the query.\n",
    "        Returns a list of the most relevant relations based on semantic similarity.\n",
    "        \"\"\"\n",
    "        relevant_relations = self.semantic_graph_search(query)\n",
    "        \n",
    "        filtered_relations = [rel for rel in relevant_relations if rel[1] > threshold]\n",
    "        \n",
    "        if len(filtered_relations) < 5 and relevant_relations:\n",
    "            threshold = 0.4  \n",
    "            filtered_relations = [rel for rel in relevant_relations if rel[1] > threshold]\n",
    "        \n",
    "\n",
    "        query_embedding = self.embedding_model.encode(query).reshape(1, -1)\n",
    "        \n",
    "        with self.driver.session() as session:\n",
    "            entities = session.run(\"MATCH (e:Entity) RETURN e.name as name, e.embedding as embedding\").data()\n",
    "            \n",
    "            if entities:\n",
    "                names = [n['name'] for n in entities]\n",
    "                embeddings = [n['embedding'] for n in entities]\n",
    "                similarities = cosine_similarity(query_embedding, embeddings)[0]\n",
    "                top_entities = [names[idx] for idx in np.argsort(similarities)[-2:][::-1]]\n",
    "                \n",
    "                for entity in top_entities:\n",
    "                    entity_rels = session.run(\"\"\"\n",
    "                        MATCH (s:Entity {name: $name})-[r:RELATIONSHIP]->(t)\n",
    "                        RETURN s.name as source, r.type as type, t.name as target\n",
    "                        UNION\n",
    "                        MATCH (s:Entity)-[r:RELATIONSHIP]->(t:Entity {name: $name})\n",
    "                        RETURN s.name as source, r.type as type, t.name as target\n",
    "                        LIMIT 5\n",
    "                    \"\"\", name=entity).data()\n",
    "                    \n",
    "                    for rel in entity_rels:\n",
    "                        rel_text = f\"{rel['source']} {rel['type']} {rel['target']}\"\n",
    "                        \n",
    "                        if not any(rel_text == existing[0] for existing in filtered_relations):\n",
    "                            filtered_relations.append((rel_text, 0.51, rel)) \n",
    "        \n",
    "        return filtered_relations[:max_relations]\n",
    "    \n",
    "    def run_pipeline(self, text):\n",
    "    \n",
    "        print(\"Extracting triplets...\")\n",
    "        raw_triplets = self.extract_triplets(text)\n",
    "        \n",
    "        print(\"Normalizing triplets...\")\n",
    "        normalized_triplets = self.normalize_triplets(raw_triplets)\n",
    "        \n",
    "        print(\"Storing in knowledge graph...\")\n",
    "        self.store_graph_embeddings(normalized_triplets)\n",
    "        self.create_vector_indexes()\n",
    "        \n",
    "        '''print(\"Generating summary...\")\n",
    "        summary = self.generate_summary()\n",
    "        \n",
    "        print(\"\\nFinal Summary:\")\n",
    "        print(summary)\n",
    "        \n",
    "        return summary'''   \n",
    "    \n",
    "    def fix_entity_references(self, text, main_entity=None):\n",
    "        if not main_entity:\n",
    "            return text\n",
    "            \n",
    "        sentences = text.split('.')\n",
    "        fixed_sentences = []\n",
    "        \n",
    "        for i, sentence in enumerate(sentences):\n",
    "            sentence = sentence.strip()\n",
    "            if not sentence:\n",
    "                continue\n",
    "                \n",
    "\n",
    "            if main_entity.lower() in sentence.lower():\n",
    "                fixed_sentences.append(sentence)\n",
    "                continue\n",
    "                \n",
    "            lower_sentence = sentence.lower()\n",
    "            if i > 0 and (lower_sentence.startswith('it ') or \n",
    "                          lower_sentence.startswith('its ') or \n",
    "                          lower_sentence.startswith('this ') or\n",
    "                          lower_sentence.startswith('the company')):\n",
    "                \n",
    "\n",
    "                if lower_sentence.startswith('it '):\n",
    "                    sentence = main_entity + sentence[2:]\n",
    "\n",
    "                elif lower_sentence.startswith('its '):\n",
    "                    sentence = main_entity + \"'s\" + sentence[3:]\n",
    "\n",
    "                elif lower_sentence.startswith('this '):\n",
    "                    sentence = main_entity + sentence[4:]\n",
    "\n",
    "                elif 'the company' in lower_sentence:\n",
    "                    sentence = sentence.lower().replace('the company', main_entity)\n",
    "\n",
    "                    sentence = sentence[0].upper() + sentence[1:]\n",
    "            \n",
    "            fixed_sentences.append(sentence)\n",
    "        \n",
    "\n",
    "        fixed_text = '. '.join(fixed_sentences)\n",
    "        if not fixed_text.endswith('.'):\n",
    "            fixed_text += '.'\n",
    "            \n",
    "        return fixed_text\n",
    "        \n",
    "    def query_knowledge_graph(self, user_query):\n",
    "        print(f\"Processing query: {user_query}\")\n",
    "        query_parts = user_query.split()\n",
    "        \n",
    "        with self.driver.session() as session:\n",
    "            entities = session.run(\"MATCH (e:Entity) RETURN e.name as name\").data()\n",
    "            entity_names = [e['name'] for e in entities]\n",
    "            \n",
    "            mentioned_entities = []\n",
    "            for entity in entity_names:\n",
    "                if entity.lower() in user_query.lower() or any(part.lower() == word.lower() \n",
    "                                                              for part in entity.split() \n",
    "                                                              for word in query_parts):\n",
    "                    mentioned_entities.append(entity)\n",
    "        \n",
    "        entity_relations = []\n",
    "        if mentioned_entities:\n",
    "            print(f\"Detected entities in query: {', '.join(mentioned_entities)}\")\n",
    "            with self.driver.session() as session:\n",
    "                for entity in mentioned_entities:\n",
    "                    subj_rels = session.run(\"\"\"\n",
    "                        MATCH (s:Entity {name: $name})-[r:RELATIONSHIP]->(t)\n",
    "                        RETURN s.name AS source, r.type AS type, t.name AS target\n",
    "                    \"\"\", name=entity).data()\n",
    "                    \n",
    "                    obj_rels = session.run(\"\"\"\n",
    "                        MATCH (s:Entity)-[r:RELATIONSHIP]->(t:Entity {name: $name})\n",
    "                        RETURN s.name AS source, r.type AS type, t.name AS target\n",
    "                    \"\"\", name=entity).data()\n",
    "                    \n",
    "                    for rel in subj_rels + obj_rels:\n",
    "                        entity_relations.append((\n",
    "                            f\"{rel['source']} {rel['type']} {rel['target']}\", \n",
    "                            1.0,  \n",
    "                            rel\n",
    "                        ))\n",
    "        semantic_relations = self.retrieve_query_relevant_relations(user_query)\n",
    "        if not entity_relations and not semantic_relations:\n",
    "            return \"I couldn't find any relevant information about that query in the knowledge graph.\"\n",
    "        all_relations = []\n",
    "        seen_relation_texts = set()\n",
    "        for rel, score, rel_dict in entity_relations:\n",
    "            if rel not in seen_relation_texts:\n",
    "                all_relations.append((rel, score, rel_dict))\n",
    "                seen_relation_texts.add(rel)\n",
    "        \n",
    "        for rel, score, rel_dict in semantic_relations:\n",
    "            if rel not in seen_relation_texts:\n",
    "                all_relations.append((rel, score, rel_dict))\n",
    "                seen_relation_texts.add(rel)\n",
    "        \n",
    "        all_relations.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        print(f\"\\nTop matching relationships:\")\n",
    "        for rel, score, _ in all_relations[:10]: \n",
    "            print(f\"- {rel} (score: {score:.3f})\")\n",
    "        \n",
    "        print(f\"\\nFound {len(all_relations)} relevant relationships:\")\n",
    "        for rel, score, _ in all_relations[:10]:  \n",
    "            print(f\"- {rel} (score: {score:.3f})\")\n",
    "        \n",
    "        summary = self.generate_summary(query=user_query, relations=all_relations)\n",
    "        \n",
    "        main_entity = mentioned_entities[0] if mentioned_entities else None\n",
    "        summary = self.fix_entity_references(summary, main_entity)\n",
    "        \n",
    "        print(f\"\\nQuery-specific summary: {summary}\")\n",
    "        return summary\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    text = \"\"\"\n",
    "    In 2022, Tesla Inc., led by CEO Elon Musk, became the world's most valuable car manufacturer, surpassing Toyota in market capitalization. The company, headquartered in Palo Alto, California, is known for its electric vehicles such as the Model S, Model 3, and Model X. Tesla's Gigafactory in Nevada plays a critical role in battery production and employs over 7,000 people.\n",
    "\n",
    "    Meanwhile, SpaceX, another company founded by Elon Musk, launched the Crew Dragon spacecraft to the International Space Station (ISS) under a partnership with NASA. The launch was conducted from the Kennedy Space Center in Florida. The ISS is jointly operated by NASA, Roscosmos, ESA, JAXA, and CSA, and orbits the Earth approximately every 90 minutes.\n",
    "\n",
    "    In the tech industry, Google, a subsidiary of Alphabet Inc., acquired Fitbit in 2021 to expand its presence in the wearable technology market. Apple Inc., on the other hand, continues to dominate the smartphone industry with the iPhone, which it manufactures in collaboration with Foxconn. Tim Cook succeeded Steve Jobs as CEO of Apple in 2011.\n",
    "\n",
    "    Across the globe, Amazon operates hundreds of fulfillment centers to ensure quick delivery of products. It uses machine learning algorithms to optimize its supply chain and predict consumer demand. Jeff Bezos founded Amazon in 1994 and stepped down as CEO in 2021, appointing Andy Jassy as his successor.\n",
    "\n",
    "    In the field of AI research, OpenAI developed GPT-4, a state-of-the-art language model capable of understanding and generating human-like text. It has been integrated into several applications, including Microsoft's Copilot and ChatGPT. Microsoft invested heavily in OpenAI, incorporating its technology into the Azure cloud platform.\n",
    "\n",
    "    In climate science, the Intergovernmental Panel on Climate Change (IPCC) published a report highlighting the urgent need to reduce greenhouse gas emissions. The Paris Agreement, signed by 196 countries in 2015, set out goals to limit global warming to below 2 degrees Celsius.\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    pipeline = KnowledgeGraphPipeline()\n",
    "    pipeline.run_pipeline(text)\n",
    "    \n",
    "\n",
    "    while True:\n",
    "        user_query = input(\"\\nEnter your query (or 'exit' to quit): \")\n",
    "        if user_query.lower() == 'exit':\n",
    "            break\n",
    "        \n",
    "        summary = pipeline.query_knowledge_graph(user_query)\n",
    "        print(f\"\\nSummary: {summary}\")\n",
    "    \n",
    "    pipeline.driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff80aa8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
